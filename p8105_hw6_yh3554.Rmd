---
title: "p8105_hw6_yh3554"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(dbplyr)
library(modelr)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1
To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

## Problem 2

#### Read the data

```{r}
homicide_df = read_csv("data_homicide/homicide-data.csv", show_col_types = FALSE)
```

#### Table of proportion of missing data

```{r}
homicide_df %>% 
  summarise_at(vars(lat:disposition), .funs = function(x) mean(is.na(x))) %>%
  knitr::kable()
```

#### Describle the raw data

The `homicide_df` is data contains homicides in 50 large U.S. It has `r nrow(homicide_df)` variables and `r ncol(homicide_df)` cases. The key variables are unique id, victim demographic information (first name, last name, age, sex), the location (city, state, latitude, longitude), and disposition. It has `r sum(is.na (homicide_df$lat))` missing latitude information and `r sum(is.na (homicide_df$lon))` missing longitude information.

#### Generate new variables

Create a city_state variable, and a binary variable indicating whether the homicide is solved. Also omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO (these donâ€™t report victim race), and Tulsa, AL (a data entry mistake). Limit analysis those for whom victim_race is white or black. Be sure that victim_age is numeric. Cannot convert victim_age to numeric variable since there is unknown age, so drop the unknown, then use `as.numeric` to convert it. Drop the unknow sex as well.
* victim_sex: male, female
* resolved: homicide case status whether it is solved
```{r}
homicide_df_new <- homicide_df %>%
  filter(!victim_age == "Unknown", 
         !victim_sex == "Unknown",
         victim_race == "White" | victim_race == "Black") %>%
  mutate(victim_age = as.numeric(victim_age),
         city_state = str_c(city, state, sep = ", "),
         resolved = as.numeric(disposition == "Closed by arrest")) %>%
  filter(!city_state == "Dallas, TX", !city_state == "Phoenix, Ax", 
         !city_state == "Kansas City, MO", !city_state == "Tulsa, AL") %>%
  mutate(victim_race = as.factor(victim_race),
         victim_sex = factor(victim_sex, level = c("Female", "Male")))

head(homicide_df_new)
summary(homicide_df_new)
```

#### Logistic regression model for city of Baltimore, MD

Use `glm` function to fit a logistic regression model\
model setup:\
dependent variable: status solved or unsolved\
predictors: victim age, victim sex, victim race\
\
```{r}
baltimore_glm <- homicide_df_new %>%
  filter(city_state == "Baltimore, MD") %>%
  select(resolved, victim_age, victim_race, victim_sex) %>%
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial(link = "logit"))

save(baltimore_glm, file = "baltimore_glm.RData")
```

#### Estimates and Confidence interval

Use `broom::tidy` to find the confidence interval of adjusted odds ratio for solving homicides comparing male victim and female victim.

```{r}
baltimore_glm  %>% 
  broom::tidy() %>% 
  mutate(lower_CI = confint(baltimore_glm)[,1],
         upper_CI = confint(baltimore_glm)[,2]) %>% 
  filter(term == 'victim_sexMale') %>% 
  select(estimate, lower_CI, upper_CI) %>% 
  mutate(estimate = exp(estimate),
         lower_CI = exp(lower_CI),
         upper_CI = exp(upper_CI))
```

The odds ratio of solving homicides for male is 0.425 higher comparing to female when all other variables fixed. Male is less likely to solve homicides comparing to female victim.

#### Losgistic regression for each cities

```{r}
homicide_city_df <- homicide_df_new %>%
  select(resolved, victim_age, victim_race, victim_sex, city_state) %>%
  nest(data = -city_state) %>%
  mutate(
    glms = map(data, ~glm(resolved ~ victim_age + victim_sex + victim_race,  data = ., family = binomial(link = "logit"))),
    results = map(glms, broom::tidy)) %>%
  select(-data, -glms) %>%
  unnest(results) %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(OR = exp(estimate),
         low_CI = estimate - std.error * qnorm(0.975),
         upper_CI = estimate + std.error * qnorm(0.975),
         OR_low_CI = exp(low_CI),
         OR_upper_CI = exp(upper_CI),
         ) %>% 
  select(city_state, OR, OR_low_CI, OR_upper_CI) 
head(homicide_city_df)
```

#### Plot of estimated ORs and CIs for each city

```{r}
homicide_plot <- homicide_city_df %>%
  ggplot(aes(x = fct_reorder(city_state, OR), y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = OR_low_CI, ymax = OR_upper_CI)) +
  labs(title = "Estimated Adjusted ORs and CIs for Male solving homocide comparing to female by city",
       x = "City",
       y = "Adjusted OR and 95% CI") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

homicide_plot
```

Based on the plot, the average estimated OR of solving homicide for most cities is less than 1, for these cities, their male victim of homicides is less likely to be resolved comparing to female. On the other hand, there are few cities has estimated OR higher than 1, they are Fresno, Stockton, and Albuquerque, however, their confidence interval are a bit wider than all other cities, so no strong evidence to say that the resovling homidecide between male and female are different.